#!/usr/bin/env python3
"""
æ¨¡å‹æ¨ç†æœåŠ¡

å€Ÿé‰´ç¦»çº¿è®­ç»ƒçš„MTLPredictoré€»è¾‘ï¼Œä½†åœ¨æœ¬é¡¹ç›®å†…å®ç°å®Œæ•´çš„æ¨¡å‹åŠ è½½å’Œæ¨ç†åŠŸèƒ½
"""

import json
import pickle
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from loguru import logger
import numpy as np

try:
    import torch
    import torch.nn as nn
    from deepctr_torch.inputs import SparseFeat, DenseFeat
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorch or DeepCTR-Torch not available")


# æ·»åŠ offline_trainingè·¯å¾„ä»¥å¯¼å…¥æ¨¡å‹å®šä¹‰
offline_path = Path(__file__).parent.parent.parent / "offline_training"
sys.path.insert(0, str(offline_path))

from app.utils.config import config
from app.models.input_models import PredictionOutput


# ========== MPS Compatibility Patch ==========
# Fix for MPS (Apple Silicon) devices: DeepCTR-Torch's BaseModel.predict()
# has multiple float64 issues that break MPS compatibility
if TORCH_AVAILABLE:
    from deepctr_torch.models.basemodel import BaseModel
    from torch.utils.data import DataLoader
    from torch.utils import data as Data

    _original_deepctr_predict = BaseModel.predict

    def _mps_safe_predict(self, x, batch_size=256):
        """MPS-compatible reimplementation of BaseModel.predict()

        Fixes multiple float64 issues:
        1. np.concatenate defaults to float64
        2. torch.from_numpy preserves float64
        3. Final .astype("float64") conversion
        """
        model = self.eval()

        # Convert dict to list of arrays
        if isinstance(x, dict):
            x = [x[feature] for feature in self.feature_index]

        # Ensure all inputs are float32 numpy arrays
        for i in range(len(x)):
            if len(x[i].shape) == 1:
                x[i] = np.expand_dims(x[i], axis=1)
            # Force float32 for MPS compatibility
            if x[i].dtype == np.float64:
                x[i] = x[i].astype(np.float32)

        # Concatenate with explicit float32 dtype
        concatenated = np.concatenate(x, axis=-1)
        if concatenated.dtype == np.float64:
            logger.debug("ğŸ”„ Converting concatenated features from float64 to float32")
            concatenated = concatenated.astype(np.float32)

        # Create tensor dataset
        tensor_data = Data.TensorDataset(torch.from_numpy(concatenated))
        test_loader = DataLoader(dataset=tensor_data, shuffle=False, batch_size=batch_size)

        # Run predictions
        pred_ans = []
        with torch.no_grad():
            for _, x_test in enumerate(test_loader):
                x_batch = x_test[0].to(self.device).float()
                y_pred = model(x_batch).cpu().data.numpy()
                pred_ans.append(y_pred)

        # Return float32 predictions instead of float64
        result = np.concatenate(pred_ans).astype(np.float32)
        logger.debug(f"âœ… MPS-safe predict completed, output dtype: {result.dtype}")
        return result

    BaseModel.predict = _mps_safe_predict
    logger.info("âœ… Applied comprehensive MPS compatibility patch to DeepCTR BaseModel")
# ========== End MPS Compatibility Patch ==========


class ModelInferenceService:
    """æ¨¡å‹æ¨ç†æœåŠ¡
    
    å€Ÿé‰´MTLPredictorçš„å®ç°é€»è¾‘ï¼Œæ”¯æŒæ ‡å‡†checkpointåŠ è½½å’Œç‰¹å¾é¢„å¤„ç†
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹æ¨ç†æœåŠ¡"""
        self.model = None
        self.preprocessors = None
        self.feature_columns = []
        self.feature_names = []
        self.tasks = []
        self.task_column_mapping = {}
        self.label_normalizer = None
        self.training_info = {}
        
        # è®¾ç½®è®¾å¤‡
        self.device = self._get_device()
        
        # checkpointç›®å½•
        self.checkpoint_dir = Path(config.MODEL_PATH).parent
        
        logger.info("="*60)
        logger.info("ğŸš€ Initializing Model Inference Service")
        logger.info(f"Checkpoint dir: {self.checkpoint_dir}")
        logger.info(f"Device: {self.device}")
        logger.info("="*60)
        
        if not TORCH_AVAILABLE:
            logger.error("âŒ PyTorch not available, cannot load model")
            return
        
        # æŒ‰ç…§MTLPredictorçš„é€»è¾‘åˆå§‹åŒ–
        try:
            # 1. åŠ è½½checkpointå…ƒæ•°æ®
            self._load_metadata()
            
            # 2. åŠ è½½æ¨¡å‹
            logger.info("Loading model...")
            self._load_model()
            
            # 3. åŠ è½½é¢„å¤„ç†å™¨
            logger.info("Loading preprocessors...")
            self._load_preprocessors()
            
            # 4. åŠ è½½ç‰¹å¾åˆ—å®šä¹‰
            logger.info("Loading feature columns...")
            self._load_feature_columns()
            
            # 5. åŠ è½½æ ‡ç­¾å½’ä¸€åŒ–å™¨
            logger.info("Loading label normalizer...")
            self._load_label_normalizer()

            # éªŒè¯ label_normalizer çŠ¶æ€
            if self.label_normalizer is None:
                logger.error("âŒ CRITICAL: Label normalizer is None!")
                logger.error("   Predictions will NOT be denormalized - this will cause wrong predictions!")
                logger.error("   Expected file: models/label_normalizer.pkl")
            else:
                logger.info("âœ… Label normalizer validation:")
                logger.info(f"   Normalization method: {getattr(self.label_normalizer, 'normalization_method', 'unknown')}")
                if hasattr(self.label_normalizer, 'fitted_tasks'):
                    logger.info(f"   Fitted tasks: {self.label_normalizer.fitted_tasks}")
                if hasattr(self.label_normalizer, 'normalizers'):
                    logger.info(f"   Number of normalizers: {len(self.label_normalizer.normalizers)}")

            # 6. åŠ è½½è®­ç»ƒä¿¡æ¯
            logger.info("Loading training info...")
            self._load_training_info()
            
            logger.info("âœ… Model Inference Service initialized successfully")
            logger.info(f"Model type: {self.training_info.get('model_type', 'unknown')}")
            logger.info(f"Tasks: {', '.join(self.tasks)}")
            
            # 7. é¢„çƒ­æ¨¡å‹
            if self.model:
                self._warmup()
                
        except Exception as e:
            logger.error(f"âŒ Failed to initialize model inference service: {e}", exc_info=True)
    
    def _get_device(self) -> str:
        """è·å–æ¨ç†è®¾å¤‡"""
        if not TORCH_AVAILABLE:
            return 'cpu'
        
        if torch.backends.mps.is_available():
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    
    def _load_metadata(self):
        """åŠ è½½checkpointå…ƒæ•°æ®"""
        metadata_file = self.checkpoint_dir / "checkpoint_metadata.json"
        if metadata_file.exists():
            # æ‰“å°æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
            file_size = metadata_file.stat().st_size
            file_mtime = metadata_file.stat().st_mtime
            import time
            mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_mtime))
            logger.info(f"ğŸ“„ Loading: {metadata_file}")
            logger.info(f"   Size: {file_size} bytes, Modified: {mtime_str}")

            with open(metadata_file, 'r') as f:
                self.metadata = json.load(f)
            logger.info(f"   âœ… Loaded checkpoint metadata: version {self.metadata.get('version', 'unknown')}")
        else:
            logger.warning("âš ï¸  No metadata file found in checkpoint")
            self.metadata = {}
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹ï¼Œå€Ÿé‰´ModelLoaderçš„é€»è¾‘"""
        # é¦–å…ˆå°è¯•åŠ è½½å®Œæ•´æ¨¡å‹
        complete_model_path = self.checkpoint_dir / "complete_model.pth"
        if complete_model_path.exists():
            try:
                logger.info(f"Loading complete model from {complete_model_path}")
                self.model = torch.load(complete_model_path, map_location=self.device)
                self.model.eval()
                logger.info(f"âœ… Complete model loaded successfully: {self.model.__class__.__name__}")
                return
            except Exception as e:
                logger.warning(f"Failed to load complete model: {e}")
                logger.info("Falling back to rebuild method...")
        
        # å›é€€åˆ°ä»é…ç½®é‡å»ºæ¨¡å‹
        self._rebuild_and_load_model()
    
    def _rebuild_and_load_model(self):
        """ä»é…ç½®é‡å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡"""
        try:
            # 1. åŠ è½½æ¨¡å‹é…ç½®
            model_config = self._load_model_config()
            
            # 2. ä¸´æ—¶åŠ è½½ç‰¹å¾åˆ—ç”¨äºæ¨¡å‹é‡å»º
            temp_feature_columns = self._load_feature_columns_for_model()
            
            # 3. é‡å»ºæ¨¡å‹
            self.model = self._create_model(model_config, temp_feature_columns)
            
            # 4. åŠ è½½æƒé‡
            weights_file = self.checkpoint_dir / "model.pth"
            if weights_file.exists():
                # æ‰“å°æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
                file_size = weights_file.stat().st_size
                file_mtime = weights_file.stat().st_mtime
                import time
                mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_mtime))
                logger.info(f"ğŸ“„ Loading model weights: {weights_file}")
                logger.info(f"   Size: {file_size / 1024 / 1024:.2f} MB, Modified: {mtime_str}")

                state_dict = torch.load(weights_file, map_location=self.device)

                # æ‰“å°æƒé‡ä¿¡æ¯
                logger.info(f"   State dict keys: {len(state_dict.keys())}")
                # æ˜¾ç¤ºå‰5ä¸ªæƒé‡çš„å½¢çŠ¶
                for i, (key, tensor) in enumerate(list(state_dict.items())[:5]):
                    logger.info(f"   [{i}] {key}: {tensor.shape}")

                self.model.load_state_dict(state_dict)
                logger.info("   âœ… Model weights loaded successfully")
            else:
                logger.error("âŒ Model weights not found!")
                return
            
            self.model.eval()
            logger.info(f"âœ… Model rebuilt successfully: {self.model.__class__.__name__}")
            
        except Exception as e:
            logger.error(f"Failed to rebuild model: {e}", exc_info=True)
            self.model = None
    
    def _load_model_config(self) -> Dict[str, Any]:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        config_file = self.checkpoint_dir / "model_config.json"
        if not config_file.exists():
            raise FileNotFoundError(f"Model config not found: {config_file}")

        # æ‰“å°æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        file_size = config_file.stat().st_size
        file_mtime = config_file.stat().st_mtime
        import time
        mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_mtime))
        logger.info(f"ğŸ“„ Loading: {config_file}")
        logger.info(f"   Size: {file_size} bytes, Modified: {mtime_str}")

        with open(config_file, 'r') as f:
            config_data = json.load(f)

        # æ‰“å°å…³é”®é…ç½®ä¿¡æ¯
        logger.info(f"   âœ… Model type: {config_data.get('model_type', 'unknown')}")
        logger.info(f"   âœ… Tasks: {config_data.get('tasks', [])}")
        logger.info(f"   âœ… L2 reg embedding: {config_data.get('l2_reg_embedding', 'N/A')}")
        logger.info(f"   âœ… L2 reg dnn: {config_data.get('l2_reg_dnn', 'N/A')}")

        return config_data
    
    def _load_feature_columns_for_model(self) -> List:
        """åŠ è½½ç‰¹å¾åˆ—å®šä¹‰ï¼ˆç”¨äºæ¨¡å‹é‡å»ºï¼‰"""
        feature_file = self.checkpoint_dir / "feature_columns.json"
        if not feature_file.exists():
            raise FileNotFoundError(f"Feature columns not found: {feature_file}")

        # æ‰“å°æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        file_size = feature_file.stat().st_size
        file_mtime = feature_file.stat().st_mtime
        import time
        mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_mtime))
        logger.info(f"ğŸ“„ Loading: {feature_file}")
        logger.info(f"   Size: {file_size} bytes, Modified: {mtime_str}")

        with open(feature_file, 'r') as f:
            feature_data = json.load(f)

        # é‡å»ºç‰¹å¾åˆ—å¯¹è±¡
        feature_columns = []
        sparse_count = 0
        dense_count = 0
        for feat_info in feature_data:
            if feat_info['type'] == 'SparseFeat':
                feature_columns.append(SparseFeat(
                    name=feat_info['name'],
                    vocabulary_size=feat_info['vocabulary_size'],
                    embedding_dim=feat_info['embedding_dim'],
                    dtype=feat_info.get('dtype', 'int32')
                ))
                sparse_count += 1
            elif feat_info['type'] == 'DenseFeat':
                feature_columns.append(DenseFeat(
                    name=feat_info['name'],
                    dimension=feat_info.get('dimension', 1),
                    dtype=feat_info.get('dtype', 'float32')
                ))
                dense_count += 1

        logger.info(f"   âœ… Loaded {len(feature_columns)} feature columns (Sparse: {sparse_count}, Dense: {dense_count})")
        return feature_columns
    
    def _create_model(self, model_config: Dict[str, Any], feature_columns: List) -> nn.Module:
        """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
        model_type = model_config.get('model_type', 'PNN_MMOE')
        
        if model_type == 'PNN_MMOE':
            # å¯¼å…¥PNN_MMOEæ¨¡å‹
            from offline_training.training.base.pnn_mmoe_model import PNN_MMOE

            # ğŸ” å…³é”®è¯Šæ–­ï¼šæ£€æŸ¥å¯¼å…¥çš„PNN_MMOEæ¨¡å‹æ¥æº
            import inspect
            pnn_module_file = inspect.getfile(PNN_MMOE)
            logger.info(f"ğŸ” PNN_MMOE model class loaded from: {pnn_module_file}")
            import os
            if os.path.exists(pnn_module_file):
                pnn_mtime = os.path.getmtime(pnn_module_file)
                import time
                pnn_mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(pnn_mtime))
                logger.info(f"   File modified: {pnn_mtime_str}")

            pnn_mmoe_config = model_config.get('pnn_mmoe_config', {})
            mmoe_config = pnn_mmoe_config.get('mmoe', {})
            pnn_config = pnn_mmoe_config.get('pnn', {})
            
            model = PNN_MMOE(
                dnn_feature_columns=feature_columns,
                num_tasks=len(model_config.get('tasks', [])),
                task_types=['binary'] * len(model_config.get('tasks', [])),
                task_names=model_config.get('tasks', []),
                use_inner_product=pnn_config.get('use_inner_product', True),
                use_outter_product=pnn_config.get('use_outter_product', False),
                num_experts=mmoe_config.get('num_experts', 3),
                expert_dnn_hidden_units=tuple(mmoe_config.get('expert_dims', [128, 64])),
                gate_dnn_hidden_units=tuple(mmoe_config.get('gate_dims', [32])),
                tower_dnn_hidden_units=tuple(mmoe_config.get('tower_dims', [64, 32])),
                dnn_dropout=model_config.get('dropout', 0.1),
                l2_reg_embedding=model_config.get('l2_reg_embedding', 1e-5),
                l2_reg_dnn=model_config.get('l2_reg_dnn', 0),
                device=self.device
            )
            
            logger.info(f"Created PNN_MMOE model with {len(model_config.get('tasks', []))} tasks")
            return model
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    def _load_preprocessors(self):
        """åŠ è½½é¢„å¤„ç†å™¨"""
        preprocessor_file = self.checkpoint_dir / "preprocessors.pkl"
        if preprocessor_file.exists():
            try:
                # æ‰“å°æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
                file_size = preprocessor_file.stat().st_size
                file_mtime = preprocessor_file.stat().st_mtime
                import time
                mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_mtime))
                logger.info(f"ğŸ“„ Loading: {preprocessor_file}")
                logger.info(f"   Size: {file_size / 1024 / 1024:.2f} MB, Modified: {mtime_str}")

                with open(preprocessor_file, 'rb') as f:
                    self.preprocessors = pickle.load(f)

                # æ‰“å°preprocessorså†…å®¹
                if isinstance(self.preprocessors, dict):
                    logger.info(f"   Keys: {list(self.preprocessors.keys())}")
                    if 'label_encoders' in self.preprocessors:
                        logger.info(f"   Label encoders count: {len(self.preprocessors['label_encoders'])}")
                    if 'scalers' in self.preprocessors:
                        logger.info(f"   Scalers count: {len(self.preprocessors['scalers'])}")

                logger.info("   âœ… Preprocessors loaded successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to load preprocessors: {e}")
                self.preprocessors = None
        else:
            logger.warning("âš ï¸  Preprocessors file not found")
            self.preprocessors = None
    
    def _load_feature_columns(self):
        """åŠ è½½ç‰¹å¾åˆ—å®šä¹‰"""
        feature_file = self.checkpoint_dir / "feature_columns.json"
        if feature_file.exists():
            try:
                with open(feature_file, 'r') as f:
                    feature_data = json.load(f)
                
                # æå–ç‰¹å¾åç§°
                self.feature_names = [feat['name'] for feat in feature_data]
                self.feature_columns = feature_data
                
                logger.info(f"âœ… Loaded {len(self.feature_names)} feature columns")
            except Exception as e:
                logger.error(f"Failed to load feature columns: {e}")
                self.feature_columns = []
                self.feature_names = []
        else:
            logger.warning("Feature columns file not found")
            self.feature_columns = []
            self.feature_names = []
    
    def _load_label_normalizer(self):
        """åŠ è½½æ ‡ç­¾å½’ä¸€åŒ–å™¨"""
        normalizer_file = self.checkpoint_dir / "label_normalizer.pkl"
        if normalizer_file.exists():
            try:
                with open(normalizer_file, 'rb') as f:
                    self.label_normalizer = pickle.load(f)
                logger.info("âœ… Label normalizer loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load label normalizer: {e}")
                self.label_normalizer = None
        else:
            logger.info("Label normalizer file not found (this is normal)")
            self.label_normalizer = None
    
    def _load_training_info(self):
        """åŠ è½½è®­ç»ƒä¿¡æ¯"""
        training_file = self.checkpoint_dir / "training_info.json"
        if training_file.exists():
            try:
                with open(training_file, 'r') as f:
                    self.training_info = json.load(f)
                
                self.tasks = self.training_info.get('tasks', [])
                self.task_column_mapping = self.training_info.get('task_column_mapping', {})
                
                logger.info(f"âœ… Training info loaded: {len(self.tasks)} tasks")
            except Exception as e:
                logger.error(f"Failed to load training info: {e}")
                self.training_info = {}
                self.tasks = []
                self.task_column_mapping = {}
        else:
            logger.warning("Training info file not found")
            self.training_info = {}
            self.tasks = []
            self.task_column_mapping = {}
    
    def _warmup(self):
        """é¢„çƒ­æ¨¡å‹ï¼Œå‡å°‘é¦–æ¬¡æ¨ç†å»¶è¿Ÿ"""
        if not self.model:
            return
        
        logger.info("ğŸ”¥ Warming up model...")
        
        try:
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            dummy_input = {}
            for feat_name in self.feature_names:
                dummy_input[feat_name] = np.zeros(1, dtype=np.float32)
            
            # æ‰§è¡Œä¸€æ¬¡æ¨ç†
            with torch.no_grad():
                _ = self.model.predict(dummy_input, batch_size=1)
            
            logger.info("âœ… Model warmed up successfully")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Model warmup failed: {e}")
    
    def predict(self, features: Dict) -> PredictionOutput:
        """
        æ‰§è¡Œæ¨¡å‹æ¨ç†
        
        Args:
            features: ç‰¹å¾å­—å…¸
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        note_id = features.get('note_id')
        
        if not self.model:
            logger.warning("Model not loaded, using mock prediction")
            return self._get_mock_prediction(note_id)
        
        try:
            logger.info(f"ğŸ”® Starting model inference for note: {note_id}")

            # é¢„å¤„ç†ç‰¹å¾
            logger.debug("Step 1: Starting _preprocess_features...")
            processed_features = self._preprocess_features(features)
            logger.debug(f"Step 1 âœ…: _preprocess_features completed, feature dtypes: {[f.dtype for f in processed_features.values() if isinstance(f, np.ndarray)][:5]}")

            # æ‰§è¡Œé¢„æµ‹
            logger.debug("Step 2: Starting model.predict...")
            with torch.no_grad():
                predictions = self.model.predict(processed_features, batch_size=1)

            # ğŸ’¾ è®°å½•æ¨¡å‹åŸå§‹è¾“å‡ºï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
            logger.info(f"ğŸ“Š Raw predictions from model (normalized space): {predictions}")
            logger.info(f"   Shape: {predictions.shape if isinstance(predictions, np.ndarray) else 'N/A'}")
            logger.info(f"   Dtype: {predictions.dtype if isinstance(predictions, np.ndarray) else type(predictions)}")
            if isinstance(predictions, np.ndarray) and predictions.ndim > 1:
                logger.info(f"   Values: {predictions[0, :]}")
            elif isinstance(predictions, np.ndarray):
                logger.info(f"   Values: {predictions}")

            logger.debug(f"Step 2 âœ…: model.predict completed")

            # Safety check: Convert any remaining float64 to float32 for MPS compatibility
            if isinstance(predictions, np.ndarray) and predictions.dtype == np.float64:
                logger.warning("âš ï¸ Predictions still in float64 after patch, converting to float32")
                predictions = predictions.astype(np.float32)

            # åå¤„ç†é¢„æµ‹ç»“æœ
            logger.debug("Step 3: Starting _postprocess_predictions...")
            result = self._postprocess_predictions(predictions, note_id)
            logger.debug("Step 3 âœ…: _postprocess_predictions completed")

            logger.info(f"âœ… Model inference completed for note: {note_id}")
            return result

        except Exception as e:
            logger.error(f"âŒ Prediction failed for note {note_id}: {e}", exc_info=True)
            return self._get_mock_prediction(note_id)
    
    def predict_batch(self, features_list: List[Dict]) -> List[PredictionOutput]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            features_list: ç‰¹å¾å­—å…¸åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not self.model:
            logger.warning("Model not loaded, using mock predictions")
            return [self._get_mock_prediction(f.get('note_id')) for f in features_list]
        
        try:
            logger.info(f"ğŸ”® Starting batch inference for {len(features_list)} notes")
            
            # é¢„å¤„ç†æ‰€æœ‰ç‰¹å¾
            batch_features = []
            for features in features_list:
                processed = self._preprocess_features(features)
                batch_features.append(processed)
            
            # åˆå¹¶ä¸ºæ‰¹é‡è¾“å…¥
            batch_input = {}
            for feat_name in self.feature_names:
                feat_values = []
                for processed in batch_features:
                    feat_values.append(processed.get(feat_name, np.array([0.0], dtype=np.float32))[0])
                batch_input[feat_name] = np.array(feat_values, dtype=np.float32)
            
            # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
            with torch.no_grad():
                batch_predictions = self.model.predict(batch_input, batch_size=len(features_list))
            
            # åå¤„ç†ç»“æœ
            results = []
            for i, features in enumerate(features_list):
                note_id = features.get('note_id')
                pred_values = batch_predictions[i] if len(batch_predictions.shape) > 1 else batch_predictions
                result = self._postprocess_single_prediction(pred_values, note_id)
                results.append(result)
            
            logger.info(f"âœ… Batch inference completed for {len(features_list)} notes")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Batch prediction failed: {e}", exc_info=True)
            return [self._get_mock_prediction(f.get('note_id')) for f in features_list]
    
    def _get_sparse_and_dense_features(self):
        """ä» feature_columns ä¸­è¯†åˆ«ç¨€ç–å’Œå¯†é›†ç‰¹å¾"""
        sparse_features = []
        dense_features = []

        for feat_info in self.feature_columns:
            feat_name = feat_info['name']
            feat_type = feat_info['type']

            if feat_type == 'SparseFeat':
                sparse_features.append(feat_name)
            elif feat_type == 'DenseFeat':
                dense_features.append(feat_name)

        return sparse_features, dense_features

    def _apply_label_encoder(self, feature_name: str, value):
        """åº”ç”¨ LabelEncoder åˆ°ç¨€ç–ç‰¹å¾"""
        try:
            if not self.preprocessors or 'label_encoders' not in self.preprocessors:
                logger.warning(f"No label encoders available, returning default for {feature_name}")
                return 0

            encoder = self.preprocessors['label_encoders'].get(feature_name)
            if encoder is None:
                logger.warning(f"No encoder found for feature {feature_name}, using default 0")
                return 0

            # å¤„ç†å­—ç¬¦ä¸²å€¼
            if isinstance(value, str):
                try:
                    encoded_value = encoder.transform([value])[0]
                    return int(encoded_value)
                except (ValueError, KeyError) as e:
                    # æœªçŸ¥ç±»åˆ«ï¼Œè¿”å›é»˜è®¤å€¼
                    logger.warning(f"Unknown category '{value}' for feature {feature_name}, using default 0")
                    return 0
            elif isinstance(value, (int, float)):
                return int(value)
            else:
                return 0

        except Exception as e:
            logger.error(f"Error encoding feature {feature_name}: {e}")
            return 0

    def _apply_standard_scaler(self, feature_name: str, value: float) -> float:
        """åº”ç”¨ StandardScaler åˆ°å¯†é›†ç‰¹å¾"""
        try:
            # âœ… FIX #2: CLIP ç‰¹å¾å·²ç»æ ‡å‡†åŒ–ï¼Œè·³è¿‡ StandardScaler
            # CLIP æ¨¡å‹è¾“å‡ºçš„ embedding ç‰¹å¾å·²ç»å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
            # å¦‚æœå†åº”ç”¨ StandardScaler ä¼šç ´åç‰¹å¾åˆ†å¸ƒ
            CLIP_FEATURE_PREFIXES = ['cover_image_feat_', 'title_feat_', 'content_feat_',
                                     'inner_image_feat_', 'tag_feat_']
            if any(feature_name.startswith(prefix) for prefix in CLIP_FEATURE_PREFIXES):
                # CLIP features are already normalized, skip scaling
                return np.float32(value)

            if not self.preprocessors or 'scalers' not in self.preprocessors:
                logger.warning(f"No scalers available, returning raw value for {feature_name}")
                return float(value)

            scaler = self.preprocessors['scalers'].get(feature_name)
            if scaler is None:
                #logger.warning(f"No scaler found for feature {feature_name}, using raw value")
                return float(value)

            # åº”ç”¨æ ‡å‡†åŒ–
            if isinstance(value, (int, float)):
                # å¼ºåˆ¶ä½¿ç”¨ float32 è¾“å…¥ï¼Œé¿å… sklearn è¿”å› float64
                scaled_value = scaler.transform(np.array([[value]], dtype=np.float32))[0][0]
                logger.info(f"scaled_value: {scaled_value}, raw value: {value}")
                # ç¡®ä¿è¿”å› float32 ä»¥å…¼å®¹ MPS è®¾å¤‡
                return np.float32(scaled_value)
            else:
                logger.warning(f"Non-numeric value for dense feature {feature_name}, using 0.0")
                return np.float32(0.0)

        except Exception as e:
            logger.error(f"Error scaling feature {feature_name}: {e}")
            return np.float32(value)

    def _apply_pca_if_needed(self, features: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """å¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨äº† PCAï¼Œåˆ™åº”ç”¨ PCA è½¬æ¢"""
        try:
            if not self.preprocessors or 'pca_transformers' not in self.preprocessors:
                return features

            pca_transformers = self.preprocessors['pca_transformers']
            if not pca_transformers:
                return features

            # å¤„ç†æ¯ä¸ª PCA ç»„
            for pca_name, pca in pca_transformers.items():
                original_features = []
                original_feat_names = []

                for feat_name in self.feature_names:
                    if feat_name.startswith(f"{pca_name}_feat_"):
                        if feat_name in features:
                            # ç¡®ä¿æ”¶é›†çš„ç‰¹å¾æ˜¯ float32
                            original_features.append(np.float32(features[feat_name][0]))
                            original_feat_names.append(feat_name)

                if original_features:
                    # åº”ç”¨ PCA - æ˜¾å¼æŒ‡å®š float32 dtype
                    original_array = np.array(original_features, dtype=np.float32).reshape(1, -1)
                    # PCA è¿”å› float64ï¼Œç«‹å³è½¬æ¢ä¸º float32
                    pca_result = pca.transform(original_array)[0].astype(np.float32)

                    # æ›¿æ¢åŸå§‹ç‰¹å¾ä¸º PCA ç‰¹å¾
                    for feat_name in original_feat_names:
                        del features[feat_name]

                    # æ·»åŠ  PCA ç‰¹å¾ï¼ˆå·²ç»æ˜¯ float32ï¼‰
                    n_components = len(pca_result)
                    for i in range(n_components):
                        pca_feat_name = f"{pca_name}_pca_{i}"
                        features[pca_feat_name] = np.array([pca_result[i]], dtype=np.float32)

                    logger.debug(f"Applied PCA for {pca_name}: {len(original_features)} â†’ {n_components} dims")

            return features

        except Exception as e:
            logger.error(f"Error applying PCA: {e}")
            return features

    def _preprocess_features(self, features: Dict) -> Dict[str, np.ndarray]:
        """
        é¢„å¤„ç†ç‰¹å¾ï¼ˆä¿®å¤ç‰ˆæœ¬ - åº”ç”¨è®­ç»ƒæ—¶çš„é¢„å¤„ç†å™¨ï¼‰

        åº”ç”¨é¡ºåº:
        1. LabelEncoder for sparse features (categorical â†’ integer)
        2. StandardScaler for dense features (normalization)
        3. PCA for CLIP features (optional dimensionality reduction)
        """
        try:
            # è¯†åˆ«ç¨€ç–å’Œå¯†é›†ç‰¹å¾
            sparse_features, dense_features = self._get_sparse_and_dense_features()

            # ========== Phase 2: ç‰¹å¾ä¸€è‡´æ€§åˆ†æ ==========
            # åˆ†æè¾“å…¥ç‰¹å¾ vs é¢„æœŸç‰¹å¾ï¼Œå¸®åŠ©å‘ç°è®­ç»ƒå’Œæ¨ç†çš„ä¸ä¸€è‡´
            expected_features = set(self.feature_names)
            provided_features = set(features.keys())
            missing_features = expected_features - provided_features
            extra_features = provided_features - expected_features

            # æŒ‰ç‰¹å¾ç±»å‹åˆ†ç»„ç¼ºå¤±ç‰¹å¾
            missing_sparse = [f for f in missing_features if f in sparse_features]
            missing_dense = [f for f in missing_features if f in dense_features]
            missing_other = [f for f in missing_features if f not in sparse_features and f not in dense_features]

            # æ‰“å°ç‰¹å¾ç»Ÿè®¡æ‘˜è¦
            logger.info(f"ğŸ“Š Feature Analysis: {len(expected_features)} expected, {len(provided_features)} provided, {len(missing_features)} missing")

            # è¯¦ç»†æ˜¾ç¤ºç¼ºå¤±ç‰¹å¾ï¼ˆé™åˆ¶è¾“å‡ºæ•°é‡ï¼‰
            if missing_sparse:
                logger.warning(f"ğŸ” Missing {len(missing_sparse)} sparse features (showing first 10): {missing_sparse[:10]}")
            if missing_dense:
                logger.warning(f"ğŸ” Missing {len(missing_dense)} dense features (showing first 10): {missing_dense[:10]}")
            if missing_other:
                logger.debug(f"ğŸ” Missing {len(missing_other)} other features (CLIP/embeddings, showing first 10): {missing_other[:10]}")

            # æ˜¾ç¤ºé¢å¤–æä¾›çš„ç‰¹å¾ï¼ˆå¯èƒ½æ˜¯æ–°å¢çš„ï¼‰
            if extra_features:
                logger.debug(f"âœ¨ Extra features provided (not in training): {list(extra_features)[:10]}")

            # æ˜¾ç¤ºå­˜åœ¨çš„å…³é”®ç‰¹å¾
            key_features = ['note_id', 'title', 'content', 'cover_image', 'nickname', 'note_type']
            present_key_features = [f for f in key_features if f in provided_features]
            logger.info(f"âœ… Present key features: {present_key_features}")
            # ========== End Phase 2 ==========

            processed = {}

            # å¤„ç†æ¯ä¸ªç‰¹å¾
            for feat_name in self.feature_names:
                if feat_name in sparse_features:
                    # ç¨€ç–ç‰¹å¾ï¼šåº”ç”¨ LabelEncoder
                    raw_value = features.get(feat_name, '')
                    encoded_value = self._apply_label_encoder(feat_name, raw_value)
                    processed[feat_name] = np.array([encoded_value], dtype=np.int32)

                elif feat_name in dense_features:
                    # å¯†é›†ç‰¹å¾ï¼šåº”ç”¨ StandardScaler
                    raw_value = features.get(feat_name, 0.0)
                    scaled_value = self._apply_standard_scaler(feat_name, raw_value)
                    # ç¡®ä¿ä½¿ç”¨ float32ï¼ˆMPS å…¼å®¹ï¼‰
                    processed[feat_name] = np.array([float(scaled_value)], dtype=np.float32)

                else:
                    # å…¶ä»–ç‰¹å¾ï¼ˆå¦‚ CLIP embeddingsï¼‰
                    if feat_name in features:
                        value = features[feat_name]
                        if isinstance(value, (list, np.ndarray)):
                            processed[feat_name] = np.array(value, dtype=np.float32).flatten()[:1]
                        elif isinstance(value, (int, float)):
                            processed[feat_name] = np.array([float(value)], dtype=np.float32)
                        else:
                            processed[feat_name] = np.array([0.0], dtype=np.float32)
                    else:
                        processed[feat_name] = np.array([0.0], dtype=np.float32)

            # åº”ç”¨ PCAï¼ˆå¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
            processed = self._apply_pca_if_needed(processed)

            # ========== Phase 2: ç‰¹å¾å¤„ç†ç»Ÿè®¡ ==========
            # ç»Ÿè®¡å„ç±»ç‰¹å¾çš„å¤„ç†æƒ…å†µ
            processed_sparse = sum(1 for f in processed.keys() if f in sparse_features)
            processed_dense = sum(1 for f in processed.keys() if f in dense_features)
            processed_other = len(processed) - processed_sparse - processed_dense

            # ç»Ÿè®¡ä½¿ç”¨é»˜è®¤å€¼çš„ç‰¹å¾æ•°é‡
            features_with_defaults = len(missing_features)
            default_rate = (features_with_defaults / len(expected_features) * 100) if expected_features else 0

            logger.info(f"âœ… Preprocessed {len(processed)} features: sparse={processed_sparse}/{len(sparse_features)}, "
                       f"dense={processed_dense}/{len(dense_features)}, other={processed_other}")
            logger.info(f"âš ï¸  Using default values for {features_with_defaults} features ({default_rate:.1f}%)")
            # ========== End Phase 2 ==========

            # ========== ç‰¹å¾è¯¦ç»†è¯Šæ–­ï¼šä¿å­˜æ‰€æœ‰ç‰¹å¾åå’Œå€¼ç”¨äºç¦»åœ¨çº¿å¯¹æ¯” ==========
            
            import json
            from pathlib import Path
            try:
                features_dict = {}
                for feat_name, feat_value in processed.items():
                    # å°†numpyæ•°ç»„è½¬ä¸ºPythonåˆ—è¡¨
                    if isinstance(feat_value, np.ndarray):
                        features_dict[feat_name] = feat_value.tolist()
                    else:
                        features_dict[feat_name] = float(feat_value) if isinstance(feat_value, (int, float)) else str(feat_value)

                # ä¿å­˜åˆ°JSONæ–‡ä»¶
                output_file = Path("online_features.json")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(features_dict, f, indent=2, ensure_ascii=False)

                logger.info(f"ğŸ“ Saved {len(features_dict)} features to {output_file}")

                # æ‰“å°å‰10ä¸ªç¨€ç–ç‰¹å¾å’Œå‰10ä¸ªå¯†é›†ç‰¹å¾ä½œä¸ºæ ·æœ¬
                sparse_sample = [f for f in processed.keys() if f in sparse_features][:10]
                dense_sample = [f for f in processed.keys() if f in dense_features][:10]

                logger.info(f"ğŸ“Š Sample sparse features ({len(sparse_sample)}):")
                for feat in sparse_sample:
                    logger.info(f"   {feat}: {features_dict[feat]}")

                logger.info(f"ğŸ“Š Sample dense features ({len(dense_sample)}):")
                for feat in dense_sample:
                    logger.info(f"   {feat}: {features_dict[feat]}")

            except Exception as e:
                logger.warning(f"Failed to save features for diagnosis: {e}")
            # ========== End ç‰¹å¾è¯Šæ–­ ==========
            
            return processed

        except Exception as e:
            logger.error(f"Feature preprocessing failed: {e}", exc_info=True)
            # è¿”å›é»˜è®¤ç‰¹å¾
            return {feat_name: np.array([0.0], dtype=np.float32) for feat_name in self.feature_names}
    
    def _postprocess_predictions(self, predictions, note_id: Optional[str] = None) -> PredictionOutput:
        """åå¤„ç†é¢„æµ‹ç»“æœ"""
        try:
            if isinstance(predictions, np.ndarray):
                pred_values = predictions.flatten()
            else:
                pred_values = np.array(predictions).flatten()
            
            return self._postprocess_single_prediction(pred_values, note_id)
            
        except Exception as e:
            logger.error(f"Postprocess failed: {e}")
            return self._get_mock_prediction(note_id)
    
    def _postprocess_single_prediction(self, pred_values, note_id: Optional[str] = None) -> PredictionOutput:
        """åå¤„ç†å•ä¸ªé¢„æµ‹ç»“æœ"""
        try:
            # æ ¹æ®ä»»åŠ¡æ˜ å°„æå–é¢„æµ‹å€¼
            task_mapping = self.task_column_mapping

            # âœ… FIX #1: åº”ç”¨ label denormalizationï¼ˆä»æ ‡å‡†åŒ–ç©ºé—´è½¬å›åŸå§‹ç©ºé—´ï¼‰
            # è®­ç»ƒæ—¶å¯¹æ ‡ç­¾åšäº† StandardScaler æ ‡å‡†åŒ–ï¼Œé¢„æµ‹å€¼ä¹Ÿæ˜¯æ ‡å‡†åŒ–åçš„
            # å¿…é¡»è¿›è¡Œé€†å˜æ¢æ‰èƒ½å¾—åˆ°çœŸå®çš„é¢„æµ‹å€¼
            logger.info(f"ğŸ“Š Raw predictions from model (normalized space): {pred_values}")
            logger.info(f"   Shape: {pred_values.shape}, Dtype: {pred_values.dtype}")
            logger.info(f"   Number of tasks: {len(self.tasks)}")
            logger.info(f"   Tasks: {self.tasks}")

            if self.label_normalizer is not None:
                logger.info(f"ğŸ”„ Applying label denormalization to predictions")
                logger.info(f"   Using normalizer: {type(self.label_normalizer).__name__}")
                logger.info(f"   Normalizer fitted tasks: {self.label_normalizer.fitted_tasks if hasattr(self.label_normalizer, 'fitted_tasks') else 'N/A'}")

                # pred_values æ˜¯ 1D æ•°ç»„ï¼Œéœ€è¦ reshape æˆ 2D (1, n_tasks)
                pred_values_2d = pred_values.reshape(1, -1)
                logger.info(f"   Reshaped to 2D: {pred_values_2d.shape}")

                # é€†æ ‡å‡†åŒ–ï¼šå°†æ ‡å‡†åŒ–åçš„å€¼è½¬å›åŸå§‹å°ºåº¦
                denormalized = self.label_normalizer.inverse_transform(pred_values_2d, self.tasks)
                logger.info(f"   Denormalized shape: {denormalized.shape}")

                # è½¬å› 1D æ•°ç»„
                pred_values = denormalized.flatten()

                logger.info(f"âœ… Denormalized predictions (original space): {pred_values}")
                # è¯Šæ–­ï¼šæ˜¾ç¤ºæ¯ä¸ªä»»åŠ¡çš„åå½’ä¸€åŒ–å‰åå¯¹æ¯”
                logger.info("   Per-task denormalization:")
                for i, task in enumerate(self.tasks):
                    if i < len(pred_values_2d[0]) and i < len(pred_values):
                        logger.info(f"     [{i}] {task}: {pred_values_2d[0][i]:.6f} â†’ {pred_values[i]:.6f}")
            else:
                logger.error("âŒ CRITICAL: No label_normalizer available!")
                logger.error("   Predictions are in NORMALIZED space (wrong scale)!")
                logger.error("   This will cause negative values and wrong magnitude!")

            # é»˜è®¤å€¼
            predictions = {
                'ctr': 0.05,
                'like_rate': 0.1,
                'fav_rate': 0.08,
                'comment_rate': 0.03,
                'share_rate': 0.02,
                'follow_rate': 0.01,
                'interaction_rate': 0.15,
                'ces_rate': 0.06,
                'impression': 8.0,
                'sort_score': 0.75
            }

            # ä»é¢„æµ‹å€¼ä¸­æå–
            logger.info("ğŸ“‹ Extracting predictions from denormalized values:")
            for i, task in enumerate(self.tasks):
                if i < len(pred_values):
                    predictions[task] = float(pred_values[i])
                    logger.info(f"   [{i}] {task}: {predictions[task]:.6f}")
                else:
                    logger.warning(f"   [{i}] {task}: index out of range (len={len(pred_values)}), using default")

            # âœ… FIX #3: é¢„æµ‹å€¼èŒƒå›´éªŒè¯å’Œä¿®æ­£
            # å¯¹äºç‡ç±»æŒ‡æ ‡ï¼ˆctr, like_rateç­‰ï¼‰ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†… [0, 1]
            logger.info("ğŸ”„ Applying clip to rate/ctr tasks [0, 1]:")
            rate_tasks = ['ctr', 'like_rate', 'fav_rate', 'comment_rate', 'share_rate',
                         'follow_rate', 'interaction_rate', 'ces_rate', 'sort_score']
            clip_count = 0
            for task in rate_tasks:
                if task in predictions:
                    # å°†å¼‚å¸¸å€¼é™åˆ¶åœ¨ [0, 1] èŒƒå›´å†…
                    original_value = predictions[task]
                    predictions[task] = max(0.0, min(1.0, predictions[task]))
                    if abs(original_value - predictions[task]) > 0.0001:  # é™ä½é˜ˆå€¼ä»¥æ•è·æ‰€æœ‰clip
                        logger.warning(f"   âš ï¸  {task}: {original_value:.6f} â†’ {predictions[task]:.6f} (CLIPPED!)")
                        clip_count += 1
                    else:
                        logger.info(f"   âœ… {task}: {predictions[task]:.6f} (no clip)")

            if clip_count > 0:
                logger.warning(f"âš ï¸  Total {clip_count} tasks were clipped!")

            # å¤„ç†impressionï¼ˆä»logè½¬æ¢ï¼‰
            impression_log = predictions.get('impression', 8.0)
            impression = np.exp(impression_log) if impression_log > 0 else 1000.0
            logger.info(f"ğŸ”„ Impression transformation: {impression_log:.6f} (log) â†’ {impression:.0f} (count)")

            # æœ€ç»ˆé¢„æµ‹ç»“æœè¯Šæ–­
            logger.info("ğŸ“‹ Final predictions (after all processing):")
            logger.info(f"   CTR: {predictions['ctr']:.6f}")
            logger.info(f"   Like rate: {predictions['like_rate']:.6f}")
            logger.info(f"   Fav rate: {predictions['fav_rate']:.6f}")
            logger.info(f"   Comment rate: {predictions['comment_rate']:.6f}")
            logger.info(f"   Share rate: {predictions['share_rate']:.6f}")
            logger.info(f"   Follow rate: {predictions['follow_rate']:.6f}")
            logger.info(f"   Interaction rate: {predictions['interaction_rate']:.6f}")
            logger.info(f"   CES rate: {predictions['ces_rate']:.6f}")
            logger.info(f"   Impression: {impression:.0f}")
            logger.info(f"   Sort score: {predictions['sort_score']:.6f}")

            return PredictionOutput(
                note_id=note_id,
                ctr=predictions['ctr'],
                like_rate=predictions['like_rate'],
                fav_rate=predictions['fav_rate'],
                comment_rate=predictions['comment_rate'],
                share_rate=predictions['share_rate'],
                follow_rate=predictions['follow_rate'],
                interaction_rate=predictions['interaction_rate'],
                ces_rate=predictions['ces_rate'],
                impression=float(impression),
                sort_score2=predictions['sort_score']
            )
            
        except Exception as e:
            logger.error(f"Single prediction postprocess failed: {e}")
            return self._get_mock_prediction(note_id)
    
    def _get_mock_prediction(self, note_id: Optional[str] = None) -> PredictionOutput:
        """è·å–æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ"""
        import random
        if note_id:
            random.seed(hash(note_id) % 2**32)
        else:
            random.seed(42)
        
        return PredictionOutput(
            note_id=note_id,
            ctr=float(random.uniform(0.01, 0.15)),
            like_rate=float(random.uniform(0.05, 0.25)),
            fav_rate=float(random.uniform(0.03, 0.20)),
            comment_rate=float(random.uniform(0.01, 0.10)),
            share_rate=float(random.uniform(0.005, 0.05)),
            follow_rate=float(random.uniform(0.001, 0.03)),
            interaction_rate=float(random.uniform(0.10, 0.40)),
            ces_rate=float(random.uniform(0.02, 0.15)),
            impression=float(random.uniform(1000, 50000)),
            sort_score2=float(random.uniform(0.5, 0.95))
        )
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            "model_loaded": self.model is not None,
            "device": self.device,
            "checkpoint_dir": str(self.checkpoint_dir),
            "preprocessors_loaded": self.preprocessors is not None,
        }
        
        if self.model:
            info.update({
                "tasks": self.tasks,
                "model_type": self.training_info.get('model_type', 'unknown'),
                "task_column_mapping": self.task_column_mapping,
                "feature_count": len(self.feature_names)
            })
        
        return info