import json
import time
import random
import asyncio
from typing import Dict, List, Optional
from loguru import logger

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI library not available, LLM service will use fallback mode")

from app.utils.config import config
from app.models.input_models import NoteInput, TagPrediction


class LLMTaggingService:
    """LLMæ ‡ç­¾é¢„æµ‹æœåŠ¡ - ä½¿ç”¨OpenAI SDK with é‡è¯•æœºåˆ¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–LLMæ ‡ç­¾æœåŠ¡"""
        self.api_key = config.OPENROUTER_API_KEY
        self.model = config.LLM_MODEL
        self.taxonomy = config.load_taxonomy_knowledge()
        
        # é‡è¯•é…ç½®
        self.max_retries = 3
        self.base_delay = 1.0  # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.max_delay = 60.0  # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if OPENAI_AVAILABLE and self.api_key:
            self.client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=self.api_key,
            )
            logger.info(f"âœ… Initialized OpenAI client for model: {self.model}")
            logger.info(f"OpenAI API key: {self.api_key}") 
        else:
            self.client = None
            if not OPENAI_AVAILABLE:
                logger.warning("OpenAI library not available")
            if not self.api_key:
                logger.warning("OpenRouter API key not configured")
            logger.warning("LLM service will use fallback mode (default tags)")
        
        logger.info(f"LLM tagging service initialized - Retry config: max={self.max_retries}, base_delay={self.base_delay}s")
    
    def _build_prompt(self, note: NoteInput) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ ‡ç­¾åˆ†ç±»åŠ©æ‰‹ã€‚è¯·æ ¹æ®å°çº¢ä¹¦ç¬”è®°çš„æ ‡é¢˜ã€å†…å®¹ä¸ºå…¶é¢„æµ‹6ä¸ªç»´åº¦çš„æ ‡ç­¾ã€‚

ç¬”è®°ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{note.title}
å†…å®¹ï¼š{note.content}

è¯·ä»ä»¥ä¸‹æ¯ä¸ªç»´åº¦ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªæ ‡ç­¾ï¼ˆå¦‚æœæ²¡æœ‰åˆé€‚çš„ï¼Œå¯ä»¥ç•™ç©ºï¼‰ï¼š

1. intention_lv1ï¼ˆä¸€çº§æ„å›¾ï¼‰ï¼š
å¯é€‰å€¼ï¼š{', '.join(self.taxonomy.get('intention_lv1', [])[:30])}...

2. intention_lv2ï¼ˆäºŒçº§æ„å›¾ï¼‰ï¼š
å¯é€‰å€¼ï¼š{', '.join(self.taxonomy.get('intention_lv2', [])[:30])}...

3. taxonomy1ï¼ˆä¸€çº§åˆ†ç±»ï¼‰ï¼š
å¯é€‰å€¼ï¼š{', '.join(self.taxonomy.get('taxonomy1', [])[:50])}...

4. taxonomy2ï¼ˆäºŒçº§åˆ†ç±»ï¼‰ï¼š
å¯é€‰å€¼ï¼š{', '.join(self.taxonomy.get('taxonomy2', [])[:300])}...

5. taxonomy3ï¼ˆä¸‰çº§åˆ†ç±»ï¼‰ï¼š
å¯é€‰å€¼ï¼š{', '.join(self.taxonomy.get('taxonomy3', [])[:500])}...

6. note_marketing_integrated_levelï¼ˆå†…å®¹è¥é”€æ„Ÿï¼‰ï¼š
å¯é€‰å€¼ï¼š{', '.join(self.taxonomy.get('note_marketing_integrated_level', []))}

è¯·ä»¥JSONæ ¼å¼è¿”å›é¢„æµ‹ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "intention_lv1": "é€‰æ‹©çš„æ ‡ç­¾",
    "intention_lv2": "é€‰æ‹©çš„æ ‡ç­¾",
    "taxonomy1": "é€‰æ‹©çš„æ ‡ç­¾",
    "taxonomy2": "é€‰æ‹©çš„æ ‡ç­¾",
    "taxonomy3": "é€‰æ‹©çš„æ ‡ç­¾",
    "note_marketing_integrated_level": "é€‰æ‹©çš„æ ‡ç­¾"
}}

æ³¨æ„ï¼š
- æ¯ä¸ªç»´åº¦åªé€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„æ ‡ç­¾
- å¦‚æœæŸä¸ªç»´åº¦æ²¡æœ‰åˆé€‚çš„æ ‡ç­¾ï¼Œå¯ä»¥è¿”å›ç©ºå­—ç¬¦ä¸²
- è¯·ä»…è¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—
"""
        return prompt
    
    async def predict_tags(self, note: NoteInput) -> TagPrediction:
        """
        é¢„æµ‹ç¬”è®°çš„æ ‡ç­¾ - ä½¿ç”¨OpenAI SDK with é‡è¯•æœºåˆ¶
        
        Args:
            note: ç¬”è®°è¾“å…¥
            
        Returns:
            æ ‡ç­¾é¢„æµ‹ç»“æœ
        """
        start_time = time.time()
        logger.info(f"ğŸ·ï¸ Starting LLM tag prediction for note: {note.note_id or 'unnamed'}")
        
        # å¦‚æœå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›é»˜è®¤æ ‡ç­¾
        if not self.client:
            logger.warning("LLM client not available, using default tags")
            return self._get_default_tags()
        
        # æ„å»ºprompt
        prompt = self._build_prompt(note)
        
        # æ‰“å°è¯¦ç»†çš„promptä¿¡æ¯
        logger.info("=" * 60)
        logger.info("ğŸ“ LLM PROMPT")
        logger.info("=" * 60)
        logger.info(f"Model: {self.model}")
        logger.info(f"Note ID: {note.note_id}")
        logger.info(f"Title: {note.title}")
        logger.info(f"Content: {note.content[:200]}...")
        #logger.info("--- Full Prompt ---")
        #logger.info(prompt)
        logger.info("=" * 60)
        
        # è¿›è¡Œé‡è¯•è°ƒç”¨
        for attempt in range(self.max_retries + 1):
            try:
                logger.info(f"ğŸ”„ LLM API call attempt {attempt + 1}/{self.max_retries + 1}")
                
                # è°ƒç”¨OpenAI API
                response = await self._call_openai_api(prompt)
                
                # è§£æå“åº”
                result = self._parse_llm_response(response)
                
                # è®°å½•æˆåŠŸä¿¡æ¯
                elapsed_time = time.time() - start_time
                #logger.info(f"âœ… LLM tag prediction successful in {elapsed_time:.2f}s after {attempt + 1} attempts")
                
                return result
                
            except Exception as e:
                error_type = type(e).__name__
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„é”™è¯¯
                if self._is_retryable_error(e) and attempt < self.max_retries:
                    delay = self._calculate_backoff_delay(attempt)
                    logger.warning(f"âš ï¸ LLM API call failed ({error_type}: {e}), retrying in {delay:.1f}s...")
                    await asyncio.sleep(delay)
                    continue
                else:
                    # æœ€ç»ˆå¤±è´¥æˆ–ä¸å¯é‡è¯•çš„é”™è¯¯
                    elapsed_time = time.time() - start_time
                    logger.error(f"âŒ LLM tag prediction failed after {attempt + 1} attempts in {elapsed_time:.2f}s")
                    logger.error(f"Final error: {error_type}: {e}")
                    return self._get_default_tags()
        
        # åº”è¯¥ä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨
        return self._get_default_tags()
    
    async def _call_openai_api(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI API"""
        try:
            completion = self.client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://github.com/xhs-ctr-project",
                    "X-Title": "XHS Content Tagging",
                },
                extra_body={},
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that classifies content accurately. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500,
                timeout=30.0
            )
            
            # è·å–å“åº”å†…å®¹
            content = completion.choices[0].message.content
            
            # æ‰“å°è¯¦ç»†çš„å“åº”ä¿¡æ¯
            logger.info("=" * 60)
            logger.info("ğŸ“¨ LLM RESPONSE")
            logger.info("=" * 60)
            #logger.info(f"Model: {completion.model}")
            #logger.info(f"Usage: {completion.usage}")
            #logger.info(f"Finish reason: {completion.choices[0].finish_reason}")
            logger.info("--- Response Content ---")
            logger.info(content)
            logger.info("=" * 60)
            
            return content
            
        except Exception as e:
            logger.error(f"OpenAI API call failed: {type(e).__name__}: {e}")
            raise
    
    def _parse_llm_response(self, content: str) -> TagPrediction:
        """è§£æLLMå“åº”å†…å®¹"""
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0]
            elif '```' in content:
                content = content.split('```')[1].split('```')[0]
            
            # è§£æJSON
            tags_dict = json.loads(content.strip())
            
            # åˆ›å»ºTagPredictionå¯¹è±¡
            result = TagPrediction(
                intention_lv1=tags_dict.get('intention_lv1', ''),
                intention_lv2=tags_dict.get('intention_lv2', ''),
                taxonomy1=tags_dict.get('taxonomy1', ''),
                taxonomy2=tags_dict.get('taxonomy2', ''),
                taxonomy3=tags_dict.get('taxonomy3', ''),
                note_marketing_integrated_level=tags_dict.get('note_marketing_integrated_level', '')
            )
            
            #logger.info(f"ğŸ“Š Parsed tags: {result.dict()}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {e}")
            logger.error(f"Raw content: {content}")
            raise ValueError(f"Invalid JSON response: {e}")
        except Exception as e:
            logger.error(f"Failed to parse LLM response: {e}")
            raise
    
    def _is_retryable_error(self, error: Exception) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
        error_message = str(error).lower()
        
        # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
        retryable_errors = [
            '429',  # Too Many Requests
            'too many requests',
            'rate limit',
            'timeout',
            'connection',
            'temporary',
            'server error',
            '500',
            '502',
            '503',
            '504'
        ]
        
        for retryable in retryable_errors:
            if retryable in error_message:
                return True
        
        return False
    
    def _calculate_backoff_delay(self, attempt: int) -> float:
        """è®¡ç®—æŒ‡æ•°é€€é¿å»¶è¿Ÿ"""
        # æŒ‡æ•°é€€é¿ï¼šbase_delay * 2^attempt + éšæœºæŠ–åŠ¨
        delay = self.base_delay * (2 ** attempt)
        
        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆé¿å…åŒæ—¶é‡è¯•ï¼‰
        jitter = random.uniform(0, delay * 0.1)
        delay += jitter
        
        # é™åˆ¶æœ€å¤§å»¶è¿Ÿ
        delay = min(delay, self.max_delay)
        
        return delay
    
    def _get_default_tags(self) -> TagPrediction:
        """è·å–é»˜è®¤æ ‡ç­¾ï¼ˆç”¨äºé”™è¯¯æƒ…å†µæˆ–æµ‹è¯•ï¼‰"""
        return TagPrediction(
            intention_lv1="åˆ†äº«",
            intention_lv2="åˆ†äº«æ—¥å¸¸",
            taxonomy1="ç”Ÿæ´»è®°å½•",
            taxonomy2="",
            taxonomy3="",
            note_marketing_integrated_level="ç”Ÿæ´»è®°å½•"
        )
    
    def validate_tags(self, tags: TagPrediction) -> bool:
        """
        éªŒè¯æ ‡ç­¾æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            tags: æ ‡ç­¾é¢„æµ‹ç»“æœ
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # TODO: å®ç°æ ‡ç­¾éªŒè¯é€»è¾‘
        # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦åœ¨å…è®¸çš„èŒƒå›´å†…
        return True